crollToCurrentProgress
CurrentFolder_GV.CurrentCell = CurrentFolder_GV.Rows(rowNo).Cells(1)

disable a radio btn
rdoCustomer.Enabled = False
rdoCustomer.Checked = False

process not found
Try to replace your initialization code with:
ProcessStartInfo info 
    = new ProcessStartInfo(@"C:\Program Files\Internet Explorer\iexplore.exe");
Using non full filepath on Process.Start only works if the file is found in System32 folder.

When UseShellExecute is false, the FileName property can be either a fully qualified path to the executable, or a simple executable name that the system will attempt to find within folders specified by the PATH environment variable.
// Run "csc.exe /r:System.dll /out:sample.exe stdstr.cs". UseShellExecute is false because we're specifying
// an executable directly and in this case depending on it being in a PATH folder. By setting
// RedirectStandardOutput to true, the output of csc.exe is directed to the Process.StandardOutput stream
// which is then displayed in this console window directly.
using (Process compiler = new Process())
{
    compiler.StartInfo.FileName = "csc.exe";
    compiler.StartInfo.Arguments = "/r:System.dll /out:sample.exe stdstr.cs";
    compiler.StartInfo.UseShellExecute = false;
    compiler.StartInfo.RedirectStandardOutput = true;
    compiler.Start();

    Console.WriteLine(compiler.StandardOutput.ReadToEnd());

    compiler.WaitForExit();
}
    
    【PyAutoGUI】Pythonでマウスとキーボードを操作する
    https://tech-blog.rakus.co.jp/entry/20220613/python
    https://gakushikiweblog.com/python_pyautogui



C# Dictionary to Json file

using Newtonsoft.Json;
Dictionary<string, string> dict = new Dictionary<string, string>()
{
    {"key1", "value1"},
    {"key2", "value2"}
};
string json = JsonConvert.SerializeObject(dict);
Console.WriteLine(json);
---------------------------------------------------
//Reference:  https://jd-bots.com/2023/02/17/convert-dictionary-to-json-object-in-c-example-code/



////////////////////////////////
Json file to C# Dictionary
var json = File.ReadAllText(@"c:\temp\job3.json");
or
string json = @"{""key1"":""value1"",""key2"":""value2""}";

var values = JsonConvert.DeserializeObject<Dictionary<string, string>>(json);
-------------------------------------

//Reference:  
https://stackoverflow.com/questions/47045964/c-sharp-json-to-dictionary-of-objects
https://stackoverflow.com/questions/1207731/how-can-i-deserialize-json-to-a-simple-dictionarystring-string-in-asp-net




Good Udemy courses
Web design
https://www.udemy.com/course/complete-vuejs-3-crash-course-composition-api-vue-router-vuex/

https://www.udemy.com/course/web-application-with-nodejs-mysql/

https://www.udemy.com/course/learn-web-development-by-building-a-blog/


AI　画像処理
CNN、RNN、人工知能Webアプリの構築-
https://www.udemy.com/course/ai-pytorch/



ルールベース
https://www.udemy.com/course/image_processing_python/


////////////////
AI画像生成の知識を身につけることで、市販の画像生成ツールがうまく生成できないNG画像を自力で作成する可能性を高める。

AIによる画像生成
https://www.udemy.com/course/image_generation/
+
 人工知能Webアプリ開発入門
https://www.udemy.com/course/7step_python_vit/



///////////////////////////////////////////
Smooth Grad-CAM++
class SmoothGradCAMpp(_GradCAM):
    """Implements a class activation map extractor as described in `"Smooth Grad-CAM++: An Enhanced Inference Level
    Visualization Technique for Deep Convolutional Neural Network Models" <https://arxiv.org/pdf/1908.01224.pdf>`_
    with a personal correction to the paper (alpha coefficient numerator).

    The localization map is computed as follows:

    .. math::
        L^{(c)}_{Smooth Grad-CAM++}(x, y) = \\sum\\limits_k w_k^{(c)} A_k(x, y)

    with the coefficient :math:`w_k^{(c)}` being defined as:

    .. math::
        w_k^{(c)} = \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W \\alpha_k^{(c)}(i, j) \\cdot
        ReLU\\Big(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)}\\Big)

    where :math:`A_k(x, y)` is the activation of node :math:`k` in the target layer of the model at
    position :math:`(x, y)`,
    :math:`Y^{(c)}` is the model output score for class :math:`c` before softmax,
    and :math:`\\alpha_k^{(c)}(i, j)` being defined as:

    .. math::
        \\alpha_k^{(c)}(i, j)
        = \\frac{\\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2}}{2 \\cdot
        \\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2} + \\sum\\limits_{a,b} A_k (a,b) \\cdot
        \\frac{\\partial^3 Y^{(c)}}{(\\partial A_k(i,j))^3}}
        = \\frac{\\frac{1}{n} \\sum\\limits_{m=1}^n D^{(c, 2)}_k(i, j)}{
        \\frac{2}{n} \\sum\\limits_{m=1}^n D^{(c, 2)}_k(i, j) + \\sum\\limits_{a,b} A_k (a,b) \\cdot
        \\frac{1}{n} \\sum\\limits_{m=1}^n D^{(c, 3)}_k(i, j)}

    if :math:`\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)} = 1` else :math:`0`. Here :math:`D^{(c, p)}_k(i, j)`
    refers to the p-th partial derivative of the class score of class :math:`c` relatively to the activation in layer
    :math:`k` at position :math:`(i, j)`, and :math:`n` is the number of samples used to get the gradient estimate.

    Please note the difference in the numerator of :math:`\\alpha_k^{(c)}(i, j)`,
    which is actually :math:`\\frac{1}{n} \\sum\\limits_{k=1}^n D^{(c, 1)}_k(i,j)` in the paper.

    Example::
        >>> from torchvision.models import resnet18
        >>> from torchcam.cams import SmoothGradCAMpp
        >>> model = resnet18(pretrained=True).eval()
        >>> cam = SmoothGradCAMpp(model, 'layer4')
        >>> scores = model(input_tensor)
        >>> cam(class_idx=100)

    Args:
        model: input model
        target_layer: name of the target layer
        num_samples: number of samples to use for smoothing
        std: standard deviation of the noise
        input_shape: shape of the expected input tensor excluding the batch dimension
    """

    def __init__(
        self,
        model: torch.nn.Module,
        target_layer: Optional[str] = None,
        num_samples: int = 4,
        std: float = 0.3,
        input_shape: Tuple[int, ...] = (3, 224, 224),
    ) -> None:

        super().__init__(model, target_layer, input_shape)
        # Model scores is not used by the extractor
        self._score_used = False

        # Input hook
        self.hook_handles.append(model.register_forward_pre_hook(self._store_input))
        # Noise distribution
        self.num_samples = num_samples
        self.std = std
        self._distrib = torch.distributions.normal.Normal(0, self.std)
        # Specific input hook updater
        self._ihook_enabled = True

    def _store_input(self, module: torch.nn.Module, input: Tensor) -> None:
        """Store model input tensor"""

        if self._ihook_enabled:
            self._input = input[0].data.clone()

    def _get_weights(self, class_idx: int, scores: Optional[Tensor] = None) -> Tensor:
        """Computes the weight coefficients of the hooked activation maps"""

        self.hook_a: Tensor
        self.hook_g: Tensor
        # Disable input update
        self._ihook_enabled = False
        # Keep initial activation
        init_fmap = self.hook_a.clone()
        # Initialize our gradient estimates
        grad_2, grad_3 = torch.zeros_like(self.hook_a), torch.zeros_like(self.hook_a)
        # Perform the operations N times
        for _idx in range(self.num_samples):
            # Add noise
            noisy_input = self._input + self._distrib.sample(self._input.size()).to(device=self._input.device)
            # Forward & Backward
            out = self.model(noisy_input)
            self.model.zero_grad()
            self._backprop(out, class_idx)

            # Sum partial derivatives
            grad_2.add_(self.hook_g.pow(2))
            grad_3.add_(self.hook_g.pow(3))

        # Reenable input update
        self._ihook_enabled = True

        # Average the gradient estimates
        grad_2.div_(self.num_samples)
        grad_3.div_(self.num_samples)

        # Alpha coefficient for each pixel
        spatial_dims = self.hook_a.ndim - 2
        alpha = grad_2 / (2 * grad_2 + (grad_3 * init_fmap).flatten(2).sum(-1)[(...,) + (None,) * spatial_dims])

        # Apply pixel coefficient in each weight
        return alpha.squeeze_(0).mul_(torch.relu(self.hook_g.squeeze(0))).flatten(1).sum(-1)

    def extra_repr(self) -> str:
        return f"target_layer='{self.target_layer}', num_samples={self.num_samples}, std={self.std}"



///////////////////////////////////////////////////
utils.py

def overlay_mask(img: Image.Image, mask: Image.Image, colormap: str = 'jet', alpha: float = 0.7) -> Image.Image:
    """Overlay a colormapped mask on a background image

    Args:
        img: background image
        mask: mask to be overlayed in grayscale
        colormap: colormap to be applied on the mask
        alpha: transparency of the background image

    Returns:
        overlayed image
    """

    if not isinstance(img, Image.Image) or not isinstance(mask, Image.Image):
        raise TypeError('img and mask arguments need to be PIL.Image')

    if not isinstance(alpha, float) or alpha < 0 or alpha >= 1:
        raise ValueError('alpha argument is expected to be of type float between 0 and 1')

    cmap = cm.get_cmap(colormap)
    # Resize mask and apply colormap
    overlay = mask.resize(img.size, resample=Image.BICUBIC)
    overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)
    # Overlay the image with the mask
    overlayed_img = Image.fromarray((alpha * np.asarray(img) + (1 - alpha) * overlay).astype(np.uint8))

    return overlayed_img

/////////////////////////////////////////////////////

runCAM.py
# ワーニングメッセージを非表示
warnings.simplefilter('ignore')
# ディレクトリの存在確認
if not os.path.exists(cfg.CAM.OUTPUT_DIR):
    os.makedirs(cfg.CAM.OUTPUT_DIR)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
# 学習済モデルのパスを定義し、モデルをロードする
model_dir = cfg.CAM.MODEL_DIR
if cfg.CAM.ARC_F:
    model = Predictor2(model_dir=model_dir)
else:
    model = Predictor(model_dir=model_dir)
# ディレクトリ内にある画像ファイル名を取得
files = glob.glob(cfg.CAM.INPUT_DIR + '/*.' + cfg.CAM.EXT)
num_5per = len(files) // 20
# 画像ファイル数が20以下だった場合の暫定処理
if num_5per == 0:
    num_5per = 1
counter = 0
num_progress = 0
for file in files:
    # 画像読み込み
    img_path = file
    pil_img = Image.open(img_path, mode='r').convert('RGB').resize(cfg.CAM.SHAPE)
    img_tensor = to_tensor(pil_img).to(device=device)
    cam_extractor = cams.SmoothGradCAMpp(model.model)
    cam_extractor._hooks_enabled = True
    model.model.zero_grad()
    scores = model.model(img_tensor.unsqueeze(0))
    # scoresが最大のクラス番号を取得
    class_idx = scores.squeeze(0).argmax().item()
    # scoresの最大値を取得
    scores_max_T = scores.squeeze(0).max(0).values
    scores_max = str(scores_max_T.to('cpu').detach().numpy().copy())
    scores_max = round(float(scores_max), 2)
    scores_max = str(scores_max)
    scores_max = scores_max.replace('.', 'n')
    activation_map = cam_extractor(class_idx, scores).cpu()
    cam_extractor._hooks_enabled = False
    heatmap = to_pil_image(activation_map, mode='F')
    gray_img = pil_img.convert('L').convert('RGB')  # 見やすいよう元画像を3chのgrayscaleに変換
    result = overlay_mask(gray_img, heatmap, alpha=float(cfg.CAM.ALPHA))
    out_name = cfg.CAM.OUTPUT_DIR + '/' + os.path.splitext(os.path.basename(img_path))[0] + '.png'
    result.save(out_name)
    counter += 1
    if counter == num_5per:
        num_progress = num_progress + 5
        if num_progress < 101:
            print('\n進捗率,' + str(num_progress))
            counter = 0


Smooth Grad CAM++
https://qiita.com/yang_null_kana/items/698383a7118f95c12cce#smooth-grad-cam-9
Smooth Grad-CAM++は，Smooth Gradの考え方とGrad-CAM(++)の考え方を組み合わせたもので，重み付けとなる部分をSmooth Gradのように入力にノイズを加えて逆伝播させたものの平均を取ることで代用しています．
入力にノイズを加える＋マップを平均化することで，よりノイズの少ないヒートマップを作成できるようになります．
基本的にはGrad-CAM++の勾配計算の部分をSmooth Gradのようにノイズを加え複数回計算した平均で代用するというものです．

Grad CAM
Grad-CAMは、CAMのGlobal Average Poolingを必要とする制限がなく、物体検出、セマンティックセグメンテーションのような分類が必要となる幅広いタスクで用いることができるため、一躍有名になりました。

Grad-CAMでは、各チャンネルの重みを勾配から計算します。対象とする特徴マップの勾配は特徴マップと同じ大きさのテンソルとなるため、そこにGlobal Average Poolingを施したものを重みとしています。勾配から重みを作る理由は、判断根拠に重要となる部分ほど大きく更新される、つまり、重要な箇所ほど大きな偏微分値を持つ前提によります。
テンソルの形に着目すると、(1,C,H,W)
の特徴マップに、誤差逆伝播法で得た(1,C,H,W)
のテンソルにGAPを施し得た(1,C,1,1)
テンソルをブロードキャストした(1,C,H,W)
テンソルを掛け合わせています。


Caption should use Ving 
////////////////////////////
!不良品流出防止　Preventing defective products from entering the market
!見かけ不良低減  Reducing false alarm
パラメーター調整 Adjecting parameters
ロット停止指示   Giving lot stop instruction
上流工程へのFB   Giving feedback to upstream process
上流工程設備     Upstream process equipments
製造技術         Manufacturing technology
!品種改善        Improvement of product types
外観             Appearance
!良品             Good product
///////////////////////////////
!NG画像           NG images
見かけ不良         False alarm
外観検査機判定     Judgment made by visual inspection machine 
!真の不良　　　　　Real defective product
致命モード不良     Fatal defect
The use case of AI image classification system

//Added by myself
!見逃し overlook
!人工目視検査 manual visual inspection
　
ViDi1台で12台分の画像を処理 
Processing 12 images with one ViDi system

LIB（MDW）をモデルにAI外観検査システムを開発・導入済（見逃し0＆過検出0.05%⇒目視検査廃止） 
The AI visual inspection system is developed and deployed to LIB in MDW (0 overlook & 0.05% false alarm ⇒ manual visual inspection is abolished)

23Fは本システムの水平展開、機能拡張開発（e.g.ﾙｰﾙﾍﾞｰｽ処理＋AI処理など）に取り組む
23F will work on horizontal expansion of this system and function expansion development (e.g. rule-based processing + AI processing, etc.)


MDWに導入したAI外観検査システム構成   Configuration of the AI visual inspection system deployed to MDW
MDWのLIB溶接不良画像の例          Welding NG images of LIB in MDW 
プログラミングが不要なノーコードAI画像処理ツールViDiを活用したAI外観検査システム
The AI visual inspection system that uses ViDi, a no-code AI image processing tool that does not require programming

ノーコード/ローコードAI外観検査システム　
No-code/low-code AI visual inspection system


AI異常検知・特性予測ｼｽﾃﾑ～  AI anomaly detection and characteristic prediction system～
数値系ノーコードAIシステム  Numerical no-code AI system
@!ノーコード機械学習ツールDatarobotを活用し、異常検知/品質予測/特性予測を行うAIシステム
The AI system that performs anomaly detection/quality prediction/characteristic prediction with the no-code machine learning tool Datarobot
or
The AI system performing anomaly detection/quality prediction/characteristic prediction uses the no-code machine learning tool Datarobot


ASK 鈴木さん　特選プローブ衝撃荷重異常検知（J8S）characteristic sorting probe abnormal shouck load detection
@!97.9%の精度で　正常/異常を判別 Distinguish between normal and abnormal with 97.9 accuracy
ノーコード機械学習ツール（Datarobot） No-code machine learning tool (Datarobot)
抵抗値とプローブ荷重の関係を調査 Investigating the relationship between resistance value and probe load
抵抗値とプローブ荷重の同時取得　Getting resistance value and probe load at the same time
異常波形　Abnormal waveform
正常波形　Normal waveform
時系列データ分類システム Time series data classification system
設備異常検知 Equipment anomaly detection
品質予測     Quality prediction
人作業定量化  Human work quantification

データ加工・前処理 Data processing/preprocessing
AI学習・予測 AI learning/prediction
異常検知システム(特選ﾌﾟﾛｰﾌﾞ衝撃荷重）開発完了。’23/4よりJ8Sで導入・評価
Development of anomaly detection system(probe abnormal shouck load detection) has been completed. Deployed and evaluated in J8S from 2023/4

特性予測PoCシステム（MLCC積層容量設計）開発完了。IM5120量産工程で活用中
Development of characteristic prediction PoC system (MLCC multilayer capacity design) has been completed. Used in the IM5120 mass production process


作業支援ｿﾘｭｰｼｮﾝ・安全支援ｼｽﾃﾑ Work support solution/Safety support system
作業習熟支援システム Work proficiency support system
AI・IoT技術を活用し、品質に影響を及ぼす作業動作の習熟を支援するシステム
A system that uses AI and IoT technology to support the proficiency of work movements that affect quality


MLCCチップ投入作業　MLCC chip insertion work
重要因子の自動抽出  Automatic extraction of important factors
最重要因子は「投入時間」（現場の判断基準と同じ） 
The most important factor is "insertion time"(Same as on-site judegment criteria)
治具の3次元位置・姿勢をｾﾝｼﾝｸﾞ
Sensing 3D position and posture of a jig

動作ﾄﾗｯｷﾝｸﾞﾓｼﾞｭｰﾙ（試作品） Motion tracking module(prototype)
G作業 Good work
NG作業 NG work
作業の速さ・位置など重要因子ごとのスコアを可視化
Visualizing score of each important factor of work like speed and position  

@!23/4～IM52/54でシステム導入・活用予定
Expected to deploy and utilize the system to IM52/54 from 23/4
or
Plan to deploy and utilize the system to IM52/54 from 23/4



23Fは作業分析自動化システム開発にも取り組む
Also working on automatic work analysis system development in 23F


AI・IoT技術を活用し、楽しさと両立しながら安全行動を促すシステム
A system that uses AI/IoT technology to urge acting safely while having fun 
23Fは各事業所展開＋行動変容技術開発に取り組む
Working on horizontal expansion of each murata site + behavior change technology development in 23F


////////////////////////////////
問題点として、複数の同じ物体を検知できないことや勾配消失問題の影響を受けることが近年の研究で分かっています[10]。

////////////////////
possibility:
cams.SmoothGradCAMpp(model.model,"XX layer") is not dones
